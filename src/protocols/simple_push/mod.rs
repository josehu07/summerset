//! Replication protocol: simple push.
//!
//! Immediately logs given command and pushes the command to some other peer
//! replicas. Upon receiving acknowledgement from all peers, executes the
//! command on the state machine and replies.

mod control;
mod durability;
mod execution;
mod messages;
mod recovery;
mod request;

use std::net::SocketAddr;
use std::path::Path;

use crate::client::{ClientApiStub, ClientCtrlStub, ClientId, GenericEndpoint};
use crate::manager::{CtrlMsg, CtrlReply, CtrlRequest};
use crate::protocols::SmrProtocol;
use crate::server::{
    ApiReply, ApiRequest, CommandId, ControlHub, ExternalApi, GenericReplica,
    ReplicaId, StateMachine, StorageHub, TransportHub,
};
use crate::utils::{Bitmap, SummersetError};

use async_trait::async_trait;

use get_size::GetSize;

use serde::{Deserialize, Serialize};

use tokio::sync::watch;
use tokio::time::Duration;

/// Configuration parameters struct.
#[derive(Debug, Clone, Deserialize)]
pub struct ReplicaConfigSimplePush {
    /// Client request batching interval in millisecs.
    pub batch_interval_ms: u64,

    /// Client request batching maximum batch size.
    pub max_batch_size: usize,

    /// Path to backing file.
    pub backer_path: String,

    /// Number of peer servers to push each command to.
    pub rep_degree: u8,
}

#[allow(clippy::derivable_impls)]
impl Default for ReplicaConfigSimplePush {
    fn default() -> Self {
        ReplicaConfigSimplePush {
            batch_interval_ms: 10,
            max_batch_size: 5000,
            backer_path: "/tmp/summerset.simple_push.wal".into(),
            rep_degree: 2,
        }
    }
}

/// WAL log entry type.
#[derive(Debug, PartialEq, Eq, Clone, Serialize, Deserialize, GetSize)]
pub(crate) enum WalEntry {
    FromClient {
        reqs: Vec<(ClientId, ApiRequest)>,
    },
    PeerPushed {
        peer: ReplicaId,
        src_inst_idx: usize,
        reqs: Vec<(ClientId, ApiRequest)>,
    },
}

/// Peer-peer message type.
#[derive(Debug, Clone, Serialize, Deserialize, GetSize)]
pub(crate) enum PushMsg {
    Push {
        src_inst_idx: usize,
        reqs: Vec<(ClientId, ApiRequest)>,
    },
    PushReply {
        src_inst_idx: usize,
        num_reqs: usize,
    },
}

/// In-memory instance containing a commands batch.
pub(crate) struct Instance {
    reqs: Vec<(ClientId, ApiRequest)>,
    durable: bool,
    pending_peers: Bitmap,
    execed: Vec<bool>,
    from_peer: Option<(ReplicaId, usize)>, // peer ID, peer inst_idx
}

/// SimplePush server replica module.
pub(crate) struct SimplePushReplica {
    /// Replica ID in cluster.
    id: ReplicaId,

    /// Total number of replicas in cluster.
    population: u8,

    /// Configuration parameters struct.
    config: ReplicaConfigSimplePush,

    /// Address string for client requests API.
    _api_addr: SocketAddr,

    /// Address string for internal peer-peer communication.
    _p2p_addr: SocketAddr,

    /// ControlHub module.
    control_hub: ControlHub,

    /// ExternalApi module.
    external_api: ExternalApi,

    /// StateMachine module.
    state_machine: StateMachine,

    /// StorageHub module.
    storage_hub: StorageHub<WalEntry>,

    /// TransportHub module.
    transport_hub: TransportHub<PushMsg>,

    /// In-memory log of instances.
    insts: Vec<Instance>,

    /// Current durable WAL log file offset.
    wal_offset: usize,
}

// SimplePushReplica common helpers
impl SimplePushReplica {
    /// Compose CommandId from instance index & command index within.
    #[inline]
    fn make_command_id(inst_idx: usize, cmd_idx: usize) -> CommandId {
        debug_assert!(inst_idx <= (u32::MAX as usize));
        debug_assert!(cmd_idx <= (u32::MAX as usize));
        ((inst_idx << 32) | cmd_idx) as CommandId
    }

    /// Decompose CommandId into instance index & command index within.
    #[inline]
    fn split_command_id(command_id: CommandId) -> (usize, usize) {
        let inst_idx = (command_id >> 32) as usize;
        let cmd_idx = (command_id & ((1 << 32) - 1)) as usize;
        (inst_idx, cmd_idx)
    }
}

#[async_trait]
impl GenericReplica for SimplePushReplica {
    async fn new_and_setup(
        api_addr: SocketAddr,
        p2p_addr: SocketAddr,
        ctrl_bind: SocketAddr,
        p2p_bind_base: SocketAddr,
        manager: SocketAddr,
        config_str: Option<&str>,
    ) -> Result<Self, SummersetError> {
        // connect to the cluster manager and get assigned a server ID
        let mut control_hub =
            ControlHub::new_and_setup(ctrl_bind, manager).await?;
        let id = control_hub.me;
        let population = control_hub.population;

        // parse protocol-specific configs
        let config = parsed_config!(config_str => ReplicaConfigSimplePush;
                                    batch_interval_ms, max_batch_size,
                                    backer_path, rep_degree)?;
        if config.batch_interval_ms == 0 {
            return logged_err!(
                "invalid config.batch_interval_ms '{}'",
                config.batch_interval_ms
            );
        }

        // setup state machine module
        let state_machine = StateMachine::new_and_setup(id).await?;

        // setup storage hub module
        let storage_hub =
            StorageHub::new_and_setup(id, Path::new(&config.backer_path))
                .await?;

        // setup transport hub module
        let mut transport_hub =
            TransportHub::new_and_setup(id, population, p2p_addr).await?;

        // ask for the list of peers to proactively connect to. Do this after
        // transport hub has been set up, so that I will be able to accept
        // later peer connections
        control_hub.send_ctrl(CtrlMsg::NewServerJoin {
            id,
            protocol: SmrProtocol::SimplePush,
            api_addr,
            p2p_addr,
        })?;
        let to_peers = if let CtrlMsg::ConnectToPeers { to_peers, .. } =
            control_hub.recv_ctrl().await?
        {
            to_peers
        } else {
            return logged_err!("unexpected ctrl msg type received");
        };

        // proactively connect to some peers, then wait for all population
        // have been connected with me
        for (peer, conn_addr) in to_peers {
            let bind_addr = SocketAddr::new(
                p2p_bind_base.ip(),
                p2p_bind_base.port() + peer as u16,
            );
            transport_hub
                .connect_to_peer(peer, bind_addr, conn_addr)
                .await?;
        }
        transport_hub.wait_for_group(population).await?;

        // setup external API module, ready to take in client requests
        let external_api = ExternalApi::new_and_setup(
            id,
            api_addr,
            Duration::from_millis(config.batch_interval_ms),
            config.max_batch_size,
        )
        .await?;

        Ok(SimplePushReplica {
            id,
            population,
            config,
            _api_addr: api_addr,
            _p2p_addr: p2p_addr,
            control_hub,
            external_api,
            state_machine,
            storage_hub,
            transport_hub,
            insts: vec![],
            wal_offset: 0,
        })
    }

    async fn run(
        &mut self,
        mut rx_term: watch::Receiver<bool>,
    ) -> Result<bool, SummersetError> {
        // recover state from durable storage WAL log
        self.recover_from_wal().await?;

        // main event loop
        let mut paused = false;
        loop {
            tokio::select! {
                // client request batch
                req_batch = self.external_api.get_req_batch(), if !paused => {
                    if let Err(e) = req_batch {
                        pf_error!("error getting req batch: {}", e);
                        continue;
                    }
                    let req_batch = req_batch.unwrap();
                    if let Err(e) = self.handle_req_batch(req_batch) {
                        pf_error!("error handling req batch: {}", e);
                    }
                },

                // durable logging result
                log_result = self.storage_hub.get_result(), if !paused => {
                    if let Err(e) = log_result {
                        pf_error!("error getting log result: {}", e);
                        continue;
                    }
                    let (action_id, log_result) = log_result.unwrap();
                    if let Err(e) = self.handle_log_result(action_id, log_result) {
                        pf_error!("error handling log result {}: {}", action_id, e);
                    }
                },

                // message from peer
                msg = self.transport_hub.recv_msg(), if !paused => {
                    if let Err(_e) = msg {
                        // NOTE: commented out to prevent console lags
                        // during benchmarking
                        // pf_error!("error receiving peer msg: {}", e);
                        continue;
                    }
                    let (peer, msg) = msg.unwrap();
                    match msg {
                        PushMsg::Push { src_inst_idx, reqs } => {
                            if let Err(e) = self.handle_push_msg(peer, src_inst_idx, reqs) {
                                pf_error!("error handling peer msg: {}", e);
                            }
                        },
                        PushMsg::PushReply { src_inst_idx, num_reqs } => {
                            if let Err(e) = self.handle_push_reply(peer, src_inst_idx, num_reqs) {
                                pf_error!("error handling peer reply: {}", e);
                            }
                        },
                    }

                },

                // state machine execution result
                cmd_result = self.state_machine.get_result(), if !paused => {
                    if let Err(e) = cmd_result {
                        pf_error!("error getting cmd result: {}", e);
                        continue;
                    }
                    let (cmd_id, cmd_result) = cmd_result.unwrap();
                    if let Err(e) = self.handle_cmd_result(cmd_id, cmd_result) {
                        pf_error!("error handling cmd result {}: {}", cmd_id, e);
                    }
                },

                // manager control message
                ctrl_msg = self.control_hub.recv_ctrl() => {
                    if let Err(e) = ctrl_msg {
                        pf_error!("error getting ctrl msg: {}", e);
                        continue;
                    }
                    let ctrl_msg = ctrl_msg.unwrap();
                    match self.handle_ctrl_msg(ctrl_msg, &mut paused).await {
                        Ok(terminate) => {
                            if let Some(restart) = terminate {
                                pf_warn!(
                                    "server got {} req",
                                    if restart { "restart" } else { "shutdown" });
                                return Ok(restart);
                            }
                        },
                        Err(e) => {
                            pf_error!("error handling ctrl msg: {}", e);
                        }
                    }
                },

                // receiving termination signal
                _ = rx_term.changed() => {
                    pf_warn!("server caught termination signal");
                    return Ok(false);
                }
            }
        }
    }

    fn id(&self) -> ReplicaId {
        self.id
    }
}

/// Configuration parameters struct.
#[derive(Debug, Deserialize)]
pub struct ClientConfigSimplePush {
    /// Which server to pick.
    pub server_id: ReplicaId,
}

#[allow(clippy::derivable_impls)]
impl Default for ClientConfigSimplePush {
    fn default() -> Self {
        ClientConfigSimplePush { server_id: 0 }
    }
}

/// SimplePush client-side module.
pub(crate) struct SimplePushClient {
    /// Client ID.
    id: ClientId,

    /// Configuration parameters struct.
    config: ClientConfigSimplePush,

    /// Control API stub to the cluster manager.
    ctrl_stub: ClientCtrlStub,

    /// API stub for communicating with the current server.
    api_stub: Option<ClientApiStub>,

    /// Base bind address for sockets connecting to servers.
    api_bind_base: SocketAddr,
}

#[async_trait]
impl GenericEndpoint for SimplePushClient {
    async fn new_and_setup(
        ctrl_base: SocketAddr,
        api_bind_base: SocketAddr,
        manager: SocketAddr,
        config_str: Option<&str>,
    ) -> Result<Self, SummersetError> {
        // connect to the cluster manager and get assigned a client ID
        pf_debug!("connecting to manager '{}'...", manager);
        let ctrl_stub =
            ClientCtrlStub::new_by_connect(ctrl_base, manager).await?;
        let id = ctrl_stub.id;

        // parse protocol-specific configs
        let config = parsed_config!(config_str => ClientConfigSimplePush;
                                    server_id)?;

        Ok(SimplePushClient {
            id,
            config,
            ctrl_stub,
            api_stub: None,
            api_bind_base,
        })
    }

    async fn connect(&mut self) -> Result<(), SummersetError> {
        // disallow reconnection without leaving
        if self.api_stub.is_some() {
            return logged_err!("reconnecting without leaving");
        }

        // ask the manager about the list of active servers
        let mut sent =
            self.ctrl_stub.send_req(Some(&CtrlRequest::QueryInfo))?;
        while !sent {
            sent = self.ctrl_stub.send_req(None)?;
        }

        let reply = self.ctrl_stub.recv_reply().await?;
        match reply {
            CtrlReply::QueryInfo {
                population,
                servers_info,
            } => {
                // find a server to connect to, starting from provided server_id
                debug_assert!(!servers_info.is_empty());
                while !servers_info.contains_key(&self.config.server_id) {
                    self.config.server_id =
                        (self.config.server_id + 1) % population;
                }
                // connect to that server
                pf_debug!(
                    "connecting to server {} '{}'...",
                    self.config.server_id,
                    servers_info[&self.config.server_id].api_addr
                );
                let bind_addr = SocketAddr::new(
                    self.api_bind_base.ip(),
                    self.api_bind_base.port() + self.config.server_id as u16,
                );
                let api_stub = ClientApiStub::new_by_connect(
                    self.id,
                    bind_addr,
                    servers_info[&self.config.server_id].api_addr,
                )
                .await?;
                self.api_stub = Some(api_stub);
                Ok(())
            }
            _ => logged_err!("unexpected reply type received"),
        }
    }

    async fn leave(&mut self, permanent: bool) -> Result<(), SummersetError> {
        // send leave notification to current connected server
        if let Some(mut api_stub) = self.api_stub.take() {
            let mut sent = api_stub.send_req(Some(&ApiRequest::Leave))?;
            while !sent {
                sent = api_stub.send_req(None)?;
            }

            while api_stub.recv_reply().await? != ApiReply::Leave {}
            pf_debug!("left current server connection");
        }

        // if permanently leaving, send leave notification to the manager
        if permanent {
            let mut sent =
                self.ctrl_stub.send_req(Some(&CtrlRequest::Leave))?;
            while !sent {
                sent = self.ctrl_stub.send_req(None)?;
            }

            while self.ctrl_stub.recv_reply().await? != CtrlReply::Leave {}
            pf_debug!("left manager connection");
        }

        Ok(())
    }

    fn send_req(
        &mut self,
        req: Option<&ApiRequest>,
    ) -> Result<bool, SummersetError> {
        match self.api_stub {
            Some(ref mut api_stub) => api_stub.send_req(req),
            None => Err(SummersetError::msg("client not set up")),
        }
    }

    async fn recv_reply(&mut self) -> Result<ApiReply, SummersetError> {
        match self.api_stub {
            Some(ref mut api_stub) => api_stub.recv_reply().await,
            None => Err(SummersetError::msg("client not set up")),
        }
    }

    fn id(&self) -> ClientId {
        self.id
    }

    fn ctrl_stub(&mut self) -> &mut ClientCtrlStub {
        &mut self.ctrl_stub
    }
}
